{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DP-PATE-SkinCancer_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GbDwkSOO1jQiQyPKPqIO9yH5_N_1hbQS",
      "authorship_tag": "ABX9TyNYemwn+x0kshmR6WYeekO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyndroy/DP_Skin_Cancer_Classification_using_PATE/blob/master/DP_PATE_SkinCancerClassifier_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnhPLVr87RDm"
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHIE9RXCmDp"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import time, os, random\n",
        "import tensorflow as tf\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# libary from pysyft needed to perform pate analysis\n",
        "from syft.frameworks.torch.dp import pate\n",
        "\n",
        "# we'll train on GPU if it is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODTkqKVQEih7"
      },
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/CancerDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDu-SgVwvJT4"
      },
      "source": [
        "class SkinCancerDataset(Dataset):\n",
        "    def __init__(self, benign_path, malignant_path, transform=None):\n",
        "\n",
        "        benign_list = [[os.path.join(benign_path, filename),'0'] for filename in os.listdir(benign_path)] \n",
        "        malignant_list = [[os.path.join(malignant_path, filename),'1'] for filename in os.listdir(malignant_path)]\n",
        "       \n",
        "        self.img_list = []\n",
        "        self.img_list = benign_list + malignant_list\n",
        "        random.shuffle(self.img_list)\n",
        "\n",
        "        self.transform = transform\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)   \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.img_list[idx][0]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = int(self.img_list[idx][1])\n",
        "        return image, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XeJAV4p13uK"
      },
      "source": [
        "data_transforms = transforms.Compose([\n",
        "    # transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "trainset = SkinCancerDataset(benign_path = './data/train/benign',\n",
        "                             malignant_path = './data/train/malignant', transform = data_transforms)\n",
        "\n",
        "testset = SkinCancerDataset(benign_path = './data/test/benign',\n",
        "                             malignant_path = './data/test/malignant' , transform = data_transforms)\n",
        "\n",
        "validset = SkinCancerDataset(benign_path = './data/valid/benign',\n",
        "                             malignant_path = './data/valid/malignant', transform = data_transforms)\n",
        "\n",
        "len(trainset),len(testset),len(validset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub1OgWcHHojC"
      },
      "source": [
        "batchsize=16\n",
        "data_loader = DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Method to display Image for Tensor\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    #print(type(image))\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "    return ax\n",
        "\n",
        "# Displaying Images and other info about the train set\n",
        "ii=0\n",
        "images, labels = next(iter(data_loader))\n",
        "print(\" Image Size\",images.size())\n",
        "print(\" Image Size\",images[ii].size())\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(16,5), ncols=5)\n",
        "for ii in range(5):\n",
        "    ax = axes[ii]\n",
        "    ax.set_title(labels[ii])\n",
        "    imshow(images[ii], ax=ax, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC7X0Z_oHZCC"
      },
      "source": [
        "# TEACHERS\n",
        "#divide train set among teachers and create dataloaders for valid and trainsets\n",
        "num_teachers = 5\n",
        "valid_per = 0.2 #20% for validation\n",
        "batch_size = 40\n",
        "\n",
        "def teacher_dataloaders(trainset=trainset, num_teachers=num_teachers, batch_size=batch_size, valid_per = 0.2):\n",
        "  trainloaders = []\n",
        "  validloaders = []\n",
        "  teacher_data_len = len(trainset) // num_teachers\n",
        "\n",
        "  # create a list of shuffled indices\n",
        "  my_list = random.sample(range(1,len(trainset)), len(trainset)-1)\n",
        "  random.shuffle(my_list)\n",
        "\n",
        "  for i in range(num_teachers):\n",
        "    # get particular subset of data\n",
        "    indice = my_list[i*teacher_data_len: (i+1)*teacher_data_len]\n",
        "    data_subset = Subset(trainset, indice)\n",
        "\n",
        "    # split into train and validation set\n",
        "    valid_size = int(len(data_subset) * valid_per)\n",
        "    train_size = len(data_subset) - valid_size\n",
        "    train_subset, valid_subset = torch.utils.data.random_split(data_subset, [train_size,valid_size])\n",
        "\n",
        "    #create data loaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "    #add dataloaders to list\n",
        "    trainloaders.append(trainloader)\n",
        "    validloaders.append(validloader)\n",
        "\n",
        "  return trainloaders, validloaders\n",
        "\n",
        "# creating dataloaders\n",
        "trainloaders, validloaders = teacher_dataloaders()\n",
        "len(trainloaders), len(validloaders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQNIxDbHSpTN"
      },
      "source": [
        "#  # STUDENT\n",
        "# split into train and validation set\n",
        "batch_size = 20\n",
        "valid_size = int(len(testset) * 0.2)\n",
        "train_size = len(testset) - valid_size\n",
        "student_train_subset, student_valid_subset = torch.utils.data.random_split(testset, [train_size,valid_size])\n",
        "\n",
        "#create data loaders\n",
        "student_train_loader = DataLoader(student_train_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "student_valid_loader = DataLoader(student_valid_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "len(student_train_loader), len(student_valid_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs-2Mik_eO-0"
      },
      "source": [
        "#Teacher Model\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__() # b, 3, 32, 32\n",
        "        layer1 = torch.nn.Sequential()\n",
        "        layer1.add_module('conv1', torch.nn.Conv2d(3, 32, 3, 1, padding=1))\n",
        "        layer1.add_module('pool1', torch.nn.MaxPool2d(2, 2))\n",
        "        layer1.add_module('relu1', torch.nn.ReLU(True))\n",
        "        layer1.add_module('drop1',torch.nn.Dropout(0.25)) \n",
        "        layer1.add_module('conv2', torch.nn.Conv2d(32,64, 3, 1, padding=1))\n",
        "        layer1.add_module('pool2', torch.nn.MaxPool2d(2, 2)) \n",
        "        layer1.add_module('relu2', torch.nn.ReLU(True))\n",
        "        layer1.add_module('drop2',torch.nn.Dropout(0.25))\n",
        "        \n",
        "        self.layer1 = layer1\n",
        "        layer4 = torch.nn.Sequential()\n",
        "        layer4.add_module('fc1', torch.nn.Linear(200704, 2))   \n",
        "           \n",
        "        self.layer4 = layer4\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.layer1(x)\n",
        "        fc_input = conv1.view(conv1.size(0), -1)\n",
        "        fc_out = self.layer4(fc_input)\n",
        "\n",
        "        return fc_out"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mpaiOxzhV8c"
      },
      "source": [
        "def train(n_epochs, trainloader, validloader, model, optimizer, criterion, use_cuda, save_path= None, is_not_teacher=False):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf\n",
        "    train_lossL = []\n",
        "    valid_lossL = []\n",
        "    train_accL = []\n",
        "    valid_accL = []\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        train_correct = 0.0\n",
        "        train_total = 0.0\n",
        "        valid_correct =0.0\n",
        "        valid_total = 0.0\n",
        "        # train the model #\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(trainloader):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # initialize weights to zero\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()    \n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "\n",
        "\n",
        "            # convert output probabilities to predicted class\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            # compare predictions to true label\n",
        "            train_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "            train_total += data.size(0)\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "        temp_loss = train_loss\n",
        "        train_lossL.append(temp_loss.cpu().numpy())  \n",
        "        \n",
        "        train_accL.append(train_acc)\n",
        "\n",
        "        # validate the model\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(validloader):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            # compare predictions to true label\n",
        "            valid_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "            valid_total += data.size(0)\n",
        "            valid_acc = 100. * valid_correct / valid_total\n",
        "        temp_loss = valid_loss\n",
        "        valid_lossL.append(temp_loss.cpu().numpy())\n",
        "\n",
        "        valid_accL.append(valid_acc) \n",
        "\n",
        "        # print training/validation statistics\n",
        "        print('Epoch: {} \\n\\tTrain Loss: {:.6f} \\tTrain Acc: {:.6f} \\n\\tValid Loss: {:.6f} \\tValid Acc: {:.6f}'.format(\n",
        "            epoch,train_loss,train_acc,valid_loss,valid_acc ))\n",
        "\n",
        "        ## save the student model if validation loss has decreased\n",
        "        if is_not_teacher:\n",
        "          if valid_loss < valid_loss_min:\n",
        "              torch.save(model.state_dict(), save_path)\n",
        "              print('\\tValidation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "              valid_loss_min,\n",
        "              valid_loss))\n",
        "              valid_loss_min = valid_loss\n",
        "\n",
        "    \n",
        "    epochs = range(1,n_epochs+1)\n",
        "    plt.plot(epochs, train_lossL, 'g', label='Training loss')\n",
        "    plt.plot(epochs, valid_lossL, 'b', label='validation loss')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()  \n",
        "            \n",
        "    epochs = range(1,n_epochs+1)\n",
        "    plt.plot(epochs, train_accL, 'g', label='Training acc')\n",
        "    plt.plot(epochs, valid_accL, 'b', label='validation acc')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()       \n",
        "\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeWED-aqhbdb"
      },
      "source": [
        "# instantiate model and move it to GPU if available\n",
        "model = SimpleCNN()\n",
        "model.to(device)\n",
        "\n",
        "#define hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)\n",
        "epochs = 13"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHeKkzJFeR8A"
      },
      "source": [
        "#Training teachers\n",
        "# Training teachers\n",
        "teacher_models = []\n",
        "i = 1\n",
        "for trainloader, validloader in zip(trainloaders, validloaders):\n",
        "  print(\" Training Teacher {}\".format(i))\n",
        "  teacher_model = train(epochs, trainloader, validloader, model, optimizer, criterion, True)\n",
        "  teacher_models.append(teacher_model)\n",
        "  i+=1\n",
        "  print(\"=\"*40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQH5vJt6eZXP"
      },
      "source": [
        "#Get private labels for training student\n",
        "# get private labels\n",
        "def student_train_labels(teacher_models, dataloader):\n",
        "  student_labels = []\n",
        "\n",
        "  # get label from each teacher\n",
        "  for model in teacher_models:\n",
        "    student_label = []\n",
        "    for images,_ in dataloader:\n",
        "      with torch.no_grad():\n",
        "        images = images.cuda()\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(torch.exp(outputs), dim=1)\n",
        "      student_label.append(preds.tolist())\n",
        "\n",
        "    # add all teacher predictions to student_labels  \n",
        "    student_label = sum(student_label, [])\n",
        "    student_labels.append(student_label)\n",
        "  return student_labels\n",
        "\n",
        "predicted_labels = student_train_labels(teacher_models, student_train_loader)     \n",
        "predicted_labels = np.array([np.array(p) for p in predicted_labels]).transpose(1, 0)\n",
        "\n",
        "# We see here that we have 5 labels for each image in our dataset\n",
        "print(predicted_labels.shape)\n",
        "# See labels of 3rd Image Scan\n",
        "print(predicted_labels[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytczgGrUeeOW"
      },
      "source": [
        "#Get private labels with the most votes count and add noise them\n",
        "def add_noise(predicted_labels, epsilon=0.001):\n",
        "  noisy_labels = []\n",
        "  for preds in predicted_labels:\n",
        "\n",
        "    # get labels with max votes\n",
        "    label_counts = np.bincount(preds, minlength=2)\n",
        "\n",
        "    # add laplacian noise to label\n",
        "    epsilon = epsilon\n",
        "    beta = 1/epsilon\n",
        "    for i in range(len(label_counts)):\n",
        "      label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "    # after adding noise we get labels with max counts\n",
        "    new_label = np.argmax(label_counts)\n",
        "    noisy_labels.append(new_label)\n",
        "\n",
        "  #return noisy_labels\n",
        "  return np.array(noisy_labels)\n",
        "\n",
        "labels_with_noise = add_noise(predicted_labels, epsilon=0.001)  \n",
        "print(labels_with_noise)\n",
        "print(labels_with_noise.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYSQLVRakU-s"
      },
      "source": [
        "import csv\n",
        "def write_csv(data):\n",
        "    with open('labels.csv', 'w') as outfile:\n",
        "        writer = csv.writer(outfile)\n",
        "        writer.writerow(data)\n",
        "\n",
        "write_csv(labels_with_noise)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59EjEh0iek0u"
      },
      "source": [
        "# Performing PATE analysis\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=predicted_labels.T, indices=labels_with_noise, noise_eps=0.001, delta=1e-5)\n",
        "print('Data dependent epsilon:', data_dep_eps)\n",
        "print('Data independent epsilon:', data_ind_eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCC4SIQ_elz8"
      },
      "source": [
        "# We have to create a new training dataloader for the student with the newly created\n",
        "# labels with noise. We have to replace the old labels with the new labels\n",
        "def new_student_data_loader(dataloader, noisy_labels, batch_size=20):\n",
        "  image_list = []\n",
        "  for image,_ in dataloader:\n",
        "    image_list.append(image)\n",
        "\n",
        "  data = np.vstack(image_list)\n",
        "  new_dataset = list(zip(data, noisy_labels))\n",
        "  new_dataloader = DataLoader(new_dataset, batch_size, shuffle=False)\n",
        "\n",
        "  return new_dataloader\n",
        "\n",
        "labeled_student_trainloader = new_student_data_loader(student_train_loader, labels_with_noise)\n",
        "len(labeled_student_trainloader),len(student_valid_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2JsXrACkbgJ"
      },
      "source": [
        "epochs = 10\n",
        "student_model = train(epochs, labeled_student_trainloader, student_valid_loader, model, optimizer, criterion, True, save_path='./models/student.pth.tar', is_not_teacher=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EEF3paIkoJG"
      },
      "source": [
        "# Normal DL Training\n",
        "epochs = 10\n",
        "normal_model = train(epochs, student_train_loader, student_valid_loader, model, optimizer, criterion, True, save_path='./models/normal.pth.tar', is_not_teacher=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_4NGHtvkqVz"
      },
      "source": [
        "# Create a dataloader for the test Dataset\n",
        "batch_size=16\n",
        "print(len(validset))\n",
        "dataloader = DataLoader(validset, batch_size=batchsize, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y67s75bnksam"
      },
      "source": [
        "# We set a seed for the dataset to prevent it from producing different values every time it is run\n",
        "seed = 3\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def test(dataloader, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    Y_true = []\n",
        "    Y_pred = []\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # update average test loss\n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        \n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        \n",
        "        temp_pred = pred \n",
        "        \n",
        "        Y_pred+=(temp_pred.cpu().numpy().transpose().tolist())\n",
        "        # print(temp_pred.cpu().numpy().transpose())\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "\n",
        "    print('\\tTest Loss: {:.6f}'.format(test_loss))\n",
        "    print('\\tTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "    Y_pred = sum(Y_pred,[])\n",
        "    \n",
        "    Y_true = [int(dataloader.dataset.img_list[i][1]) for i in range(len(dataloader.dataset.img_list)) ]\n",
        "    \n",
        "    print(confusion_matrix(Y_true,Y_pred))\n",
        "\n",
        "# call test function\n",
        "print(\"Student Model\")     \n",
        "test(dataloader, student_model, criterion, True)\n",
        "\n",
        "\n",
        "print(\"\\n=======================\\nNormal Model\")\n",
        "test(dataloader, normal_model, criterion, True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}